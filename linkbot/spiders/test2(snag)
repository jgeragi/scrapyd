import urlparse
import re
from scrapy.contrib.spiders import CrawlSpider, Rule
from scrapy.selector import Selector
from scrapy.contrib.linkextractors.sgml import SgmlLinkExtractor
from linkbot.items import LinkbotItem


class LinkSpider(CrawlSpider):
    name = 'links'
    allowed_domains = ['snagajob.com']
    start_urls = ['http://www.snagajob.com/find-jobs-by-job-title?j=a']  
    REDIRECT_MAX_TIMES = 3
    COOKIES_ENABLED = False
    AUTOTHROTTLE_ENABLED = True
    CONCURRENT_REQUESTS = 600
    RETRY_ENABLED = False
    DOWNLOAD_TIMEOUT = 10
    DOWNLOAD_DELAY = 0.15
    
    rules = (
		Rule(SgmlLinkExtractor(restrict_xpaths=('//div[@class="grid-16"]/h2/a')),follow=True,callback='parse_html'),
                #Rule(SgmlLinkExtractor(restrict_xpaths=('/html/body/div[3]/div/div[1]/div/div[1]/div[2]/div[4]/nav/a')),follow=True,callback='parse_html'),
                Rule(SgmlLinkExtractor(restrict_xpaths=('/html/body/div[3]/div/div[1]/div/p[2]/a')),follow=True,callback='parse_html'),
                Rule(SgmlLinkExtractor(restrict_xpaths=('/html/body/div[3]/div/div[1]/div/div[1]/ul/li/a')),follow=True,callback='parse_html'),
                Rule(SgmlLinkExtractor(restrict_xpaths=('/html/body/div[3]/div/div[1]/div/div[2]/ul/li/a')),follow=True,callback='parse_html'),
                #Rule(SgmlLinkExtractor(allow_domains='snagajob.com'),follow=True,callback='parse_html'),
                #Rule(SgmlLinkExtractor(restrict_xpaths=('//body')),follow=True,callback='parse_html'),
	    )
    def parse_html(self, response):
        sel = Selector(response)
        links = re.findall(r'(http?://www.snagajob.com/job-seeker\S+)', response.url)
        item = LinkbotItem(
            page_links=links,
            )
        yield item


